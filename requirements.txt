# Core Deep Learning
torch>=2.0.0
torchvision>=0.15.0
torchaudio>=2.0.0

# HuggingFace Ecosystem
transformers>=4.35.0
tokenizers>=0.15.0
datasets>=2.15.0
accelerate>=0.25.0
peft>=0.7.0

# Tokenization
sentencepiece>=0.1.99
tiktoken>=0.5.0
regex>=2023.0.0              # For BPE tokenizer

# Training Optimization
bitsandbytes>=0.41.0        # 8-bit optimizers, quantization
flash-attn>=2.3.0; sys_platform != 'darwin'  # FlashAttention (Linux/Windows only)
triton>=2.1.0; sys_platform != 'darwin'      # Triton kernels (Linux only)

# Optional: Distributed Training
deepspeed>=0.12.0; sys_platform == 'linux'   # DeepSpeed (Linux only)

# Evaluation & Benchmarks
lm-eval>=0.4.0              # LM Evaluation Harness
sacrebleu>=2.3.0            # Translation metrics
rouge-score>=0.1.2          # Summarization metrics
nltk>=3.8                   # NLP utilities

# Data Processing
numpy>=1.24.0
scipy>=1.10.0
pandas>=2.0.0
pdfplumber>=0.10.0          # PDF text extraction
PyPDF2>=3.0.0               # PDF reading (fallback)

# Experiment Tracking & Visualization
wandb>=0.16.0               # Weights & Biases
tensorboard>=2.15.0
matplotlib>=3.7.0
seaborn>=0.12.0

# Configuration Management
omegaconf>=2.3.0            # Hierarchical configs
hydra-core>=1.3.0           # Config composition
pyyaml>=6.0

# Progress & Logging
tqdm>=4.66.0
rich>=13.0.0
loguru>=0.7.0

# Code Quality & Testing
pytest>=7.4.0
pytest-cov>=4.1.0
black>=23.0.0
flake8>=6.1.0
mypy>=1.7.0
isort>=5.12.0

# Jupyter & Notebooks
jupyter>=1.0.0
ipykernel>=6.25.0
ipywidgets>=8.1.0

# Utilities
requests>=2.31.0
tqdm>=4.66.0
fire>=0.5.0                 # CLI interface
python-dotenv>=1.0.0        # Environment variables

# Optional: Advanced Inference
# vllm>=0.2.0; sys_platform == 'linux'  # Fast inference (uncomment if needed)
# xformers>=0.0.22            # Memory efficient attention (uncomment if needed)

# Development Tools
pre-commit>=3.5.0
jupytext>=1.15.0            # Sync notebooks with .py files
